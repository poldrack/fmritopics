# ---
# jupyter:
#   jupytext:
#     text_representation:
#       extension: .py
#       format_name: percent
#       format_version: '1.3'
#       jupytext_version: 1.14.5
#   kernelspec:
#     display_name: Python 3 (ipykernel)
#     language: python
#     name: python3
# ---

# %% [markdown]
# Run topic modeling for each year

# %%

# code generated by ChatGPT


import pickle
import os
from bertopic import BERTopic
from bertopic.representation import OpenAI
from sentence_transformers import SentenceTransformer
from umap import UMAP
from hdbscan import HDBSCAN
from sklearn.feature_extraction.text import CountVectorizer
from bertopic.representation import KeyBERTInspired
from bertopic.vectorizers import ClassTfidfTransformer
import argparse
import openai
import pandas as pd

def load_data(datadir='data', minyear=1990, maxyear=2023, verbose=False) -> (list, list):
    """
    Load data from pickle files

    Parameters
    ----------
    datadir : str
        Directory where data is stored
    minyear : int
        Minimum year to load
    maxyear : int
        One more than the maximum year to load

    Returns
    -------
    sentences : list of str
        List of sentences

    """

    ldadir = os.path.join(datadir, 'lda_models')
    if not os.path.exists(ldadir):
        os.makedirs(ldadir)

    sentences = []
    years = []

    years_to_process = list(range(minyear, maxyear))
    # load sentence data
    for year in years_to_process:
        if verbose:
            print('loading data for year %s' % year)
        abstract_file = os.path.join(
            datadir, f'bigrammed_cleaned_abstracts_{year}.pkl'
        )
        if not os.path.exists(abstract_file):
            print('File %s does not exist' % abstract_file)
            continue
        with open(abstract_file, 'rb') as f:
            new_sentences = [' '.join(i) for i in pickle.load(f)]
            sentences = sentences + new_sentences
            years = years + [year] * len(new_sentences)

    assert len(sentences) > 0
    assert len(sentences) == len(years)

    return sentences, years


def get_embeddings(sentences, overwrite=False,
                   model_name='all-MiniLM-L6-v2'):
    embedding_model = SentenceTransformer(model_name)
    if os.path.exists('data/embeddings.pkl') and not overwrite:
        print('using existing embeddings from data/embeddings.pkl')
        with open('data/embeddings.pkl', 'rb') as f:
            embeddings = pickle.load(f)
    else:
        
        embeddings = embedding_model.encode(sentences, show_progress_bar=False)
        with open('data/embeddings.pkl', 'wb') as f:
            pickle.dump(embeddings, f)
    return embeddings, embedding_model

if __name__ == '__main__':

    argparser = argparse.ArgumentParser()
    argparser.add_argument('--min_cluster_size', type=int, default=50)
    argparser.add_argument('--n_neighbors', type=int, default=15)
    argparser.add_argument('--year', type=int, default=None)
    argparser.add_argument('--reduce_topics', action='store_true')
    argparser.add_argument('--use_gpt4', action='store_true')
    argparser.add_argument('--overwrite', action='store_true')
    args = argparser.parse_args()


    model_name = f'model-bertopic_minclust-{args.min_cluster_size}_nneighbors-{args.n_neighbors}'  # fit plain model first
    if args.year is not None:
        model_name = model_name + f'_{args.year}'
        args.min_cluster_size = 20

    modeldir = 'models'
    if not os.path.exists(modeldir):
        os.makedirs(modeldir)

    sentences, years = load_data(minyear=1990, maxyear=2023)

    # Step 1 - Extract embeddings
    embeddings, embedding_model = get_embeddings(sentences)

    # Step 2 - Reduce dimensionality
    # ala https://maartengr.github.io/BERTopic/faq.html#i-have-too-many-topics-how-do-i-decrease-them
    umap_model = UMAP(
        n_neighbors=args.n_neighbors, n_components=5, min_dist=0.0, metric='cosine'
    )

    # Step 3 - Cluster reduced embeddings
    hdbscan_model = HDBSCAN(
        min_cluster_size=args.min_cluster_size,
        metric='euclidean',
        cluster_selection_method='eom',
        prediction_data=True,
    )

    # Step 4 - Tokenize topics
    vectorizer_model = CountVectorizer(stop_words='english')

    # Step 5 - Create topic representation
    ctfidf_model = ClassTfidfTransformer()

    # Step 6 - (Optional) Fine-tune topic representations with
    # a `bertopic.representation` model
    representation_model = KeyBERTInspired()

    if args.reduce_topics:
        nr_topics = 'auto'
    else:
        nr_topics = None

    # All steps together
    topic_model = BERTopic(
        verbose=True,
        embedding_model=embedding_model,  # Step 1 - Extract embeddings
        umap_model=umap_model,  # Step 2 - Reduce dimensionality
        hdbscan_model=hdbscan_model,  # Step 3 - Cluster reduced embeddings
        vectorizer_model=vectorizer_model,  # Step 4 - Tokenize topics
        ctfidf_model=ctfidf_model,  # Step 5 - Extract topic words
        representation_model=representation_model,  # Step 6 - Fine-tune topic represenations
        nr_topics=nr_topics,
        calculate_probabilities=False
    )

    topics, probs = topic_model.fit_transform(sentences)
    df = pd.DataFrame({"Document": sentences, "Topic": topics})

    # need to exclude embedding model as it causes GPU/CPU conflict
    topic_model.save(
        os.path.join(modeldir, model_name),
        serialization='pytorch',
        save_ctfidf=True,
        save_embedding_model=True,
    )
    df.to_csv(os.path.join(modeldir, model_name + '.csv'))

    rep_docs = topic_model.get_representative_docs()
    pd.DataFrame(rep_docs).to_csv(os.path.join(modeldir, model_name + '_rep_docs.csv'))

    if args.use_gpt4:
        # now run with GPT-4 representatoin model
        with open('openai_api_key.txt', 'r') as f:
            openai.api_key = f.read().strip()

        representation_model = OpenAI(
            model='gpt-4', chat=True, exponential_backoff=True
        )

        topic_model.update_topics(sentences, representation_model=representation_model)
        
        model_name += '_gpt4'
        topics, probs = topic_model.transform(sentences)
        df = pd.DataFrame({"Document": sentences, "Topic": topics})
        topic_model.save(
            os.path.join(modeldir, model_name),
            serialization='pytorch',
            save_ctfidf=True,
            save_embedding_model=True,
        )
        df.to_csv(os.path.join(modeldir, model_name + '.csv'))

